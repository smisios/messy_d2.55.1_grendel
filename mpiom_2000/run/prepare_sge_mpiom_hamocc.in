#! /bin/ksh

######################################
#       PREPARE DIRECTORIES
#       AND RUN SCRIPT
######################################

# Supported queuing system
#   one of TWS (Tivoli Workload Scheduler - IBM), SGE (Sun Grid Engine), 
#   NQS (Network Queuing System - NEC SX), or none
# @todo Currently also used for determining use of local files,
# @todo and for choice of parallel environment
queuing=@QUEUE_TYPE@
submit=@QUEUE_SUBMIT@
host=@host@

# computing platform/site settings and compiler:
case $queuing in
  TWS) mode=SMT ; task_geometry=no ;;
esac
if [ -x `which basename 2>/dev/null | grep -v 'no .* in'` ]; then
  compiler=`basename "@FC@"`
else
  compiler=`echo "@FC@" | sed 's:^.*/::'`
fi

# name of the experiment and grid resolution:
#EXPNO=toyl10
#GRID=TOY
#LEV=L10

#EXPNO=GR30L40
#GRID=GR30
#LEV=L40

#EXPNO=tp10l40
#GRID=TP10
#LEV=L40

EXPNO=tp04l80
GRID=TP04
LEV=L80

# Pick your own working directory, e.g. $WRKSHR or /scratch/local1/$LOGNAME
# Default setting uses a user subdirectory in the default project area.
GROUP=$(id -gn)
WORKDIR=${WORK:-/work/$GROUP}/$LOGNAME

# absolute path to work directory:
EXPDIR=${WORKDIR}/experiments

# absolute path to directory with plenty of space:
ARCDIR=${WORKDIR}/experiments

# absolute path to model binary:
MODDIR=@abs_top_builddir@/bin

# model binary:
MODBIN=mpiom.x

# absolute path to directory with initial data:
INITIAL_DATA=@POOL_ROOT@/MPIOM

nproca=16    # tasks in x direction
nprocb=16    # tasks in y direction
nthread=1    # openmp threads

ncpu=$(( nproca * nprocb * nthread ))

#
# hamocc
#
hamocc=@ENABLE_HAMOCC_RUN@

#
# profiling
#
profile=no

#
# time control
#
nfixYearLen=365           # length of the year (-1, 360, 365)
ndays=0                   # number of days per run
nmonts=12                 # number of months per run
nyears=0                  # number of years per run
model_start=0000-01-01    # initial date of simulation (YYYY-MM-DD)
model_end=0001-01-01      # final date of simulation (YYYY-MM-DD)

#
# forcing data (NCEP, ERA, OMIP)
#
cforcdata=OMIP                   # forcing data set
forcing_frequency=86400          # distance between 2 nearest forcing times (seconds) 
forcing_start=0000-01-01         # forcing start year (YYYY-MM-DD)
forcing_periodicity=-999         # number of available years of forcing data set
lspat_interp_forcing=false       # spatial interpolation onto MPIOM grid
ltime_interp_forcing=false       # linear time interpolation to the current model time
lperiodic_forcing=false           # rewind forcing files to initial point at turn of year
ldebug_forcing=true              # debug mode
lwrite_forcing=true              # write interpolated forcing fields for checking
# use OMIP runoff data with NCEP forcing
[[ "${cforcdata}" = "NCEP" ]] && ldiff_runoff_grid=true || ldiff_runoff_grid=false


# the directories for the experiment will be created, if not already there
WRKDIR=${EXPDIR}/${EXPNO}
ARCHIVE=${ARCDIR}/${EXPNO}

if [ ! -d $WRKDIR ]; then
    @MKDIR_P@ ${WRKDIR}
    cd ${WRKDIR}
#   cp ${MODDIR}/${MODBIN} ${MODBIN}  &&  MODDIR=${WRKDIR}
fi
if [ ! -d $ARCHIVE ]; then
    @MKDIR_P@ $ARCHIVE/restart
    @MKDIR_P@ $ARCHIVE/outdata
fi

if [ -x '/client/bin/ksh' ]; then
  # where an override for ksh is installed, use that
  shell='/client/bin/ksh'
else
  shell='/bin/ksh'
fi

case ${queuing} in
  (TWS)
  [[ ${ncpu} -gt 32 ]] || mode=ST
  if [[ "${mode}" = "SMT" ]]; then
    [[ ${ncpu} -ge 64 ]] && tasks_per_node=64 ||  tasks_per_node=${ncpu}
    affinity=cpu
    memory_mb=750
  else
    [[ ${ncpu} -ge 32 ]] && tasks_per_node=32 ||  tasks_per_node=${ncpu}
    affinity=core
    memory_mb=1500
  fi
  ;;
esac

if [ x$task_geometry = xyes ]; then
  case ${queuing} in

    (TWS)
    # tasks per node in x direction
    if [ $nproca -ge 8 ]; then tilex=8 ;
    else tilex=$nproca ; fi
    (( tiley =  tasks_per_node / tilex ))   # tasks per node in y direction
    if [ $tiley -gt $nprocb ]; then
      if [ $nprocb -ge 8 ]; then tiley=8 ;
      else tiley=$nprocb ; fi
      (( tilex =  tasks_per_node / tiley ))   # tasks per node in y direction
    fi
    export TASK_GEOMETRY=`@abs_top_builddir@/contrib/aix/geometry.pl ${nproca}-${nprocb}-${tasks_per_node}-${tilex}-${tiley}`
    (( ncpu = nproca * nprocb ))
    (( node = ncpu / tasks_per_node )) || { node=1 ; tasks_per_node=${ncpu} ; }

    ;;
  esac
else
  case ${queuing} in
    (TWS)
    ((node = (nproca * nprocb + tasks_per_node - 1) / tasks_per_node ))
    ;;
  esac
fi

CDIDEF="@CDI@" # empty string if CDI is enabled

jobname=${EXPNO}.job
cat > ${WRKDIR}/${jobname}<<EOF1
#! ${shell}
#-----------------------------------------------------------------------------
$(case ${queuing} in

  (TWS)
printf "
# Version for LoadLeveler
#
# @ shell = ${shell}
# @ class = cluster
# @ job_type = parallel
# @ node_usage= not_shared
# @ rset = rset_mcm_affinity
$(if [[ "${task_geometry}" = "yes" ]]; then
  echo "# @ mcm_affinity_options = mcm_accumulate"
  echo "# @ task_geometry = $TASK_GEOMETRY"
else
  echo "# @ mcm_affinity_options = mcm_distribute,mcm_mem_req"
  echo "# @ node = ${node}"
  echo "# @ tasks_per_node = ${tasks_per_node}"
fi)
# @ resources = ConsumableMemory(${memory_mb}mb)
# @ task_affinity = ${affinity}(${nthread})
# @ network.MPI = sn_all,not_shared,us
# @ wall_clock_limit = 08:00:00
# @ job_name = ${jobname}
# @ output = \$(job_name).o\$(jobid)
# @ error = \$(job_name).o\$(jobid)
# @ notification = error
# @ queue"
     ;;

  (SGE)
printf "
# Version for Sun GridEngine
#
#$ -S ${shell}
#$ -N ${jobname}
#$ -o \$JOB_NAME.o\$JOB_ID
#$ -j y
#$ -cwd
#$ -pe orte ${ncpu}"
     ;;

  (NQS)
printf  "
# Version for NQSII
#
#PBS -S ${shell}
#PBS -N ${jobname}             # job name
#PBS -l cpunum_prc=${ncpu}     # 8 cpus per node
#PBS -l cputim_job=04:00:00    # 2 h realtime per node
#PBS -l memsz_job=5gb          # 48 GB Memory per node
#PBS -j o                      # join err and out to out"
     ;;

esac)

#-----------------------------------------------------------------------------
#
#                  Job file to run MPIOM with ${cforcdata} forcing
#
#-----------------------------------------------------------------------------
#
# If a command has a non-zero exit status, execute ERR trap, if set, and exit
#
set -ex
#
#=============================================================================
$(case ${queuing} in
  (TWS)
printf '
export MEMORY_AFFINITY=MCM
export MP_SINGLE_THREAD=yes

# also see hpct_guide.pdf
export HPM_EVENT_SET=92,127
export HPM_UNIQUE_FILE_NAME=yes
export HPM_AGGREGATE=average.so   # write prof. with per event set averaged values
#export HPM_AGGREGATE=mirror.so   # write unchanged profiles from indiv. tasks
#export HPM_AGGREGATE=single.so   # write profile from one task
                                  # (set via env var HPM_PRINT_TASK)

#export PREATTACH_PROCESSOR_CORE_LIST=AUTO_SELECT
'
  ;;

  (SGE)
printf '
export FORT_BUFFERD="Y"
'
  ;;

  (NQS)
printf '
export MPIPROGINF=ALL_DETAIL
#export F_PROGINF=DETAIL
export F_SETBUF=4096
'
  ;;

esac)

export OMP_NUM_THREADS=1
export MPIOM_THREADS=$nthread
export MPIEXPORT="MPIOM_THREADS"

#
# hamocc
#
hamocc=${hamocc}

#
# profiling
#
profile=${profile}

#
# time control
#
nfixYearLen=${nfixYearLen}   # length of the year (-1, 360, 365)
ndays=${ndays}
nmonts=${nmonts}
nyears=${nyears}
model_start=${model_start}
model_end=${model_end}

#
# forcing time control
#
forcing_frequency=${forcing_frequency}
forcing_start=${forcing_start}
forcing_periodicity=${forcing_periodicity}


EXPNO=${EXPNO}
echo "Experiment: \${EXPNO}"

nprocx=$nproca
nprocy=$nprocb

(( ncpus = nprocx * nprocy ))
#
echo "   CPUs: \${ncpus} (nprocx: \${nprocx}, nprocy: \${nprocy})"

#-----------------------------------------------------------------------------
#
WRKDIR=${EXPDIR}/\${EXPNO}

# absolute path to model binary and model binary
MODDIR=${MODDIR}
MODBIN=${MODBIN}

# absolute path to directory with plenty of space:
ARCHIVE=${ARCDIR}/\${EXPNO}

# absolute path to directory with initial data:
INITIAL_DATA=${INITIAL_DATA}

# horizontal and vertical resolution
GRID=${GRID}
LEV=${LEV}

#-----------------------------------------------------------------------------
caulapuv=0.005
ibolk=500
ltidal=.false.
iter_sor=300
rtsorpar=-999.
iter_sor_hack=0
rtsorpar_hack=-999.
iocad=3
iocaduv=3

if [ "\${GRID}" = "GR60" ] ; then
  ie=60
  je=50
  DT=10800
  tp=false

elif [ "\${GRID}" = "TOY" ] ; then
  ie=66
  je=36
  DT=10800
  tp=true

elif [ "\${GRID}" = "GR30" ] ; then
  ie=122
  je=101
  DT=8640
  tp=false

elif [ "\${GRID}" = "GR15" ] ; then
  ie=256
  je=220
  DT=4800
  tp=false

elif [ "\${GRID}" = "TP10" ] ; then
  ie=362
  je=192
  [[ "\${ltidal}" = ".true." ]] && DT=3600 || DT=5400
  tp=true

elif [ "\${GRID}" = "TP04" ] ; then
  ie=802
  je=404
  DT=3600
  tp=true
  caulapuv=0.00375
  iter_sor=300
  rtsorpar=1.916
  iter_sor_hack=10
  rtsorpar_hack=0.7

elif [ "\${GRID}" = "TP6M" ] ; then
  ie=3602
  je=2394
  DT=900
  tp=true
  caulapuv=0.00375
  ibolk=0
  iter_sor=1200
  rtsorpar=1.934
  iter_sor_hack=10
  rtsorpar_hack=0.7
  iocad=8
  iocaduv=8

fi


if [ "\${LEV}" = "L3" ] ; then
  ke=3

elif [ "\${LEV}" = "L10" ] ; then
  ke=10

elif [ "\${LEV}" = "L20" ] ; then
  ke=20

elif [ "\${LEV}" = "L40" ] ; then
  ke=40

elif [ "\${LEV}" = "L80" ] ; then
  ke=80
fi


#-----------------------------------------------------------------------------
#
cd \${WRKDIR}           #  output and rerun files are written into \$ARCHIVE
#
pwd
#-----------------------------------------------------------------------------
#
# specification of files
#
#-----------------------------------------------------------------------------
#
set +e

#CP='cp -p'
#CP='ln -sf'
CP='@CDO@ -b 64B copy'

$(case "${host}-${compiler}" in
  (*?86*-*-linux-*-nagfor|*?86*-*-linux-*-nagf95)
    #for NAG compiler we need little endian
    echo "CP='@CDO@ -b 64L copy'"
    ;;
esac)

\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}_arcgri             arcgri
\cp  \${INITIAL_DATA}/\${GRID}/\${GRID}_topo                topo
\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}_anta               anta
\cp \${INITIAL_DATA}/\${GRID}/\${GRID}_BEK                   BEK
chmod u+rw BEK

\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}\${LEV}_INITEM_PHC  INITEM
\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}\${LEV}_INISAL_PHC  INISAL
\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}\${LEV}_SURSAL_PHC  SURSAL

\$CP  \${INITIAL_DATA}/runoff_obs                         runoff_obs
\$CP  \${INITIAL_DATA}/runoff_pos                         runoff_pos

$(if [[ "${cforcdata}" = "OMIP" ]]; then
    if [[ "${lspat_interp_forcing}" = "true" ]]; then
      printf '
OMIP_DATA=/pool/data/MPIOM/setup/omip_365_era15
$CP  ${OMIP_DATA}/land_sea_mask.ECMWF.nc                OMIP_LSM
$CP  ${OMIP_DATA}/east_west_stress.nc                   OMIP_WIX
$CP  ${OMIP_DATA}/north_south_stress.nc                 OMIP_WIY
$CP  ${OMIP_DATA}/2m_temp_arctic_corr.nc                OMIP_TEM
$CP  ${OMIP_DATA}/total_precipitation.nc                OMIP_PREC
$CP  ${OMIP_DATA}/total_solar_radiation.nc              OMIP_SWRAD
$CP  ${OMIP_DATA}/2m_dewpoint_temp_arctic_corr.nc       OMIP_TDEW
$CP  ${OMIP_DATA}/scalar_wind.nc                        OMIP_WIND10
$CP  ${OMIP_DATA}/total_cloud_cover.nc                  OMIP_CLOUD
$CP  ${OMIP_DATA}/runoff.nc                             OMIP_RIV '
    else
      printf '
$CP  ${INITIAL_DATA}/${GRID}/${GRID}_GIWIX_OMIP365      GIWIX
$CP  ${INITIAL_DATA}/${GRID}/${GRID}_GIWIY_OMIP365      GIWIY
$CP  ${INITIAL_DATA}/${GRID}/${GRID}_GITEM_OMIP365      GITEM
$CP  ${INITIAL_DATA}/${GRID}/${GRID}_GIPREC_OMIP365     GIPREC
$CP  ${INITIAL_DATA}/${GRID}/${GRID}_GISWRAD_OMIP365    GISWRAD
$CP  ${INITIAL_DATA}/${GRID}/${GRID}_GITDEW_OMIP365     GITDEW
$CP  ${INITIAL_DATA}/${GRID}/${GRID}_GIU10_OMIP365      GIU10
$CP  ${INITIAL_DATA}/${GRID}/${GRID}_GICLOUD_OMIP365    GICLOUD
$CP  ${INITIAL_DATA}/${GRID}/${GRID}_GIRIV_OMIP365      GIRIV'
    fi
fi)

#-----------------------------------------------------------------------------
if [ \${hamocc} == "true" ] ; then
# monthly mean dust field for hamocc

\cp  \${INITIAL_DATA}/\${GRID}/\${GRID}_MAHOWALDDUST.nc      INPDUST.nc
  chmod u+rw INPDUST.nc
fi

chmod u+rw arcgri topo anta BEK GI* INITEM INISAL SURSAL

#-----------------------------------------------------------------------------
set -e

for lll in 1 ; do

if [[ -f rerun_mpiom ]]; then
  restart=yes
  # file model_date.asc is updated by MPIOM at the end of each run
  # it contains 'run_start' 'run_end' and 'next_run_start' in YYYY-MM-DD format
  if [[ -f model_date.asc ]]; then
    model_date=\$(<model_date.asc)
    run_start=\$(echo \${model_date} | awk '{print \$3}')  # start date of actual run
  else
    run_start=\${model_start}
  fi
  STA=3   # start from restart files
  RES=0   # no restoring
else
  run_start=\${model_start}
  STA=2   # start from Levitus
  RES=1   # restoring to annual mean t,s during the first run
fi
echo \${run_start}

(( ntime = 86400 / forcing_frequency ))

$(if [[ "${cforcdata}" = "NCEP" ]]; then
  printf '%s' '
NCEP_DATA=/pool/data/MPIOM/setup/ncep
OMIP_DATA=/pool/data/MPIOM/setup/omip_365_era15    # use OMIP runoff data

$CP ${NCEP_DATA}/land.sfc.gauss.nc                 NCEP_LSM

# use OMIP runoff data
#
$CP ${OMIP_DATA}/land_sea_mask.ECMWF.nc            NCEP_LSM_RUNOFF
@CDO@ seltimestep,365 ${OMIP_DATA}/runoff.nc runoff_1day.nc    # select last day
@CDO@ mergetime -settaxis,0000-01-01,00:00,1day ${OMIP_DATA}/runoff.nc \
              -setdate,0000-12-31 runoff_1day.nc \
              -setdate,0001-01-01 runoff_1day.nc omip_366_era15_runoff.nc  # append 2 days
if [[ ${ntime} -gt 1 ]]; then  # time interpolation
  @CDO@ intntime,${ntime} omip_366_era15_runoff.nc omip_366_era15_runoff.${ntime}.nc
else # daily forcing
  @CDO@ copy omip_366_era15_runoff.nc omip_366_era15_runoff.${ntime}.nc
fi

typeset -Z4 year yearend
year=${run_start%-??-??}
forcing_start_year=${forcing_start%-??-??}
[[ ${nyears} -gt 0 ]] && (( yearend = year + nyears )) || (( yearend = year + 1 ))

while [[ ${year} -le ${yearend} ]]; do
  (( forcing_year = year + forcing_start_year - ${model_start%-??-??} ))
  if [[ ${forcing_periodicity} -gt 0 ]]; then
    (( forcing_year_data = forcing_start_year + (forcing_year-forcing_start_year)%forcing_periodicity ))
  else
    forcing_year_data=${forcing_year}
  fi
  $CP ${NCEP_DATA}/uflx.sfc.gauss.${forcing_year_data}.nc       NCEP_WIX_${forcing_year}
  $CP ${NCEP_DATA}/vflx.sfc.gauss.${forcing_year_data}.nc       NCEP_WIY_${forcing_year}
  $CP ${NCEP_DATA}/air.2m.gauss.${forcing_year_data}.nc         NCEP_TEM_${forcing_year}
  $CP ${NCEP_DATA}/prate.sfc.gauss.${forcing_year_data}.nc      NCEP_PREC_${forcing_year}
  $CP ${NCEP_DATA}/dswrf.sfc.gauss.${forcing_year_data}.nc      NCEP_SWRAD_${forcing_year}
  $CP ${NCEP_DATA}/tdew.sfc.gauss.${forcing_year_data}.nc       NCEP_TDEW_${forcing_year}
  $CP ${NCEP_DATA}/wnd.10m.gauss.${forcing_year_data}.nc        NCEP_WIND10_${forcing_year}
  $CP ${NCEP_DATA}/tcdc.eatm.gauss.${forcing_year_data}.nc      NCEP_CLOUD_${forcing_year}
  $CP omip_366_era15_runoff.${ntime}.nc                         NCEP_RIV_${forcing_year}   # use OMIP runoff data
  #$CP ${NCEP_DATA}/runof.sfc.gauss.${forcing_year_data}.nc      NCEP_RIV_${forcing_year}
  #$CP ${NCEP_DATA}/pres.sfc.gauss.${forcing_year_data}.nc       NCEP_PRESS_${forcing_year}
  #$CP ${NCEP_DATA}/dlwrf.sfc.gauss.${forcing_year_data}.nc      NCEP_LWRAD_${forcing_year}
  #$CP ${NCEP_DATA}/uwnd.10m.gauss.${forcing_year_data}.nc       NCEP_U10_${forcing_year}
  #$CP ${NCEP_DATA}/vwnd.10m.gauss.${forcing_year_data}.nc       NCEP_V10_${forcing_year}

  (( year = year + 1 ))
done
'
elif [[ "${cforcdata}" = "ERA" ]]; then
  printf '%s' '
ERA_DATA=/work/im0454/ERAIN

$CP  ${ERA_DATA}/ERAIN_LSM_172.grb       ERA_LSM

typeset -Z4 year yearend
year=${run_start%-??-??}
forcing_start_year=${forcing_start%-??-??}
[[ ${nyears} -gt 0 ]] && (( yearend = year + nyears )) || (( yearend = year + 1 ))

while [[ ${year} -le ${yearend} ]]; do
  (( forcing_year = year + forcing_start_year - ${model_start%-??-??} ))
  if [[ ${forcing_periodicity} -gt 0 ]]; then
    (( forcing_year_data = forcing_start_year + (forcing_year-forcing_start_year)%forcing_periodicity ))
  else
    forcing_year_data=${forcing_year}
  fi
  $CP  ${ERA_DATA}/ERAIN_EWSS_180_${forcing_year_data}.grb        ERA_WIX_${forcing_year}
  $CP  ${ERA_DATA}/ERAIN_NSSS_181_${forcing_year_data}.grb        ERA_WIY_${forcing_year}
  $CP  ${ERA_DATA}/ERAIN_2T_167_${forcing_year_data}.grb          ERA_TEM_${forcing_year}
  $CP  ${ERA_DATA}/ERAIN_TP_142_143_${forcing_year_data}.grb      ERA_PREC_${forcing_year}
  $CP  ${ERA_DATA}/ERAIN_SSR_176_${forcing_year_data}.grb         ERA_SWRAD_${forcing_year}
  $CP  ${ERA_DATA}/ERAIN_2D_168_${forcing_year_data}.grb          ERA_TDEW_${forcing_year}
  $CP  ${ERA_DATA}/ERAIN_10WIND_165_166_${forcing_year_data}.grb  ERA_WIND10_${forcing_year}
  $CP  ${ERA_DATA}/ERAIN_TCC_164_${forcing_year_data}.grb         ERA_CLOUD_${forcing_year}
  $CP  ${ERA_DATA}/ERAIN_RO_205_${forcing_year_data}.grb          ERA_RIV_${forcing_year}

  (( year = year + 1 ))
done
'
fi)

#
# istart=0 >NEWSTART USE ONLY FOR COMPLETELY NEW SETUP (new topography etc!!!)
# istart=1 start from horizonally uniform t,s profiles
# istart=2 start from levitus
# istart=3 start from existing restart files rerun_mpiom, rerun_hamocc (default)
#
# i3drest=0 NO 3d-restoring (default)
# i3drest=1 restoring to annual mean t,s
# i3drest=2 restoring to monthluy mean t,s
#
# Advection schemes
# iocad=3 ADPO (default)
# iocad=4 obsolete, use iocad=3 and ibbl_transport=1
# iocad=5 ADFS
#
# BBL transport (slope convection)
# ibbl_transport=1  (default)
# nfixYearLen = -1, 365 or 360 ; default -1 for 365/366 (leap years) 
#
# ltidal=.true.  enables eph.tidal sub model (default=false)
#
# imean is usually superceded by ioctl namelist below
# (imean=0 no output
#  imean=1 daily average of output fields
#  imean=2 monthly average of output fields (default)
#  imean=3 yearly average of output fields
#  imean=4 every time step)
#
# nyears: number of years to be simulated (default=0)
# nmonths: number of months to be simulated (default=1)
#

model_start=\$(echo \${model_start} | sed -e "s/-/,/g" -e "s/^,/-/"),0,0,0
forcing_start=\$(echo \${forcing_start} | sed -e "s/-/,/g" -e "s/^,/-/"),0,0,0

cat > OCECTL  << EOF
&ocedim
 ie_g = \${ie}
 je_g = \${je}
 ke = \${ke}
 lbounds_exch_tp =  .\${tp}.
/
&nprocs
 nprocx=\${nprocx}
 nprocy=\${nprocy}
/
 &ocectl
 dt      = \${DT}
 caulapts= 0.
 caulapuv= \${caulapuv}
 aus     = 0.
 cah00   = 1000.
 ibolk   = \${ibolk}
 dv0     = 0.2e-2
 av0     = 0.2e-2
 cwt     = 0.5e-3
 cwa     = 0.75e-3
 cstabeps= 0.03
 dback   = 1.05e-5
 aback   = 5.e-5
 crelsal = 3.3e-7
 creltem = 0.
 cdvocon = 0.1
 cavocon = 0.0
 nfixYearLen = \${nfixYearLen}
 ltidal  = \${ltidal}
 lswr_jerlov = .true.
 jerlov_atten = 0.12
 jerlov_bluefrac = 0.28
 nyears  = \${nyears}
 nmonts  = \${nmonts}
 ndays   = \${ndays}
 imean   = 0       ! 2 for monthly mean output
 istart  = \${STA}
 i3drest = \${RES}
 ihalo_sor   = 2
 imod    = 1
 lmpitype = .false.
 lnonblock = .true.
 icontro = 0
 iocad = \${iocad}
 iocaduv = \${iocaduv}
 ibbl_transport = 1
 lundelayed_momentum_advection = .false.
 rleadclose=0.25,1,0
 h0=0.5
 iter_sor = \${iter_sor}
 rtsorpar = \${rtsorpar}
 iter_sor_hack = \${iter_sor_hack}
 rtsorpar_hack = \${rtsorpar_hack}
 model_start_time = \${model_start}
 time_verbosity = 1
 lzo_correct = .true.
 lsaoclose = .true.
/
EOF

if [ \${ke} -eq 3 ] ; then
cat >> OCECTL  << EOF2
 &ocedzw
 cdzw     = 12.,10.,5000.,
 /
EOF2
fi

if [ \${ke} -eq 10 ] ; then
cat >> OCECTL  << EOF2
&ocedzw
  cdzw = 40., 50., 90., 160., 270., 390., 550., 900., 1300., 2300.
 /
EOF2
fi

if [ \${ke} -eq 20 ] ; then
cat >> OCECTL  << EOF2
 &ocedzw
  cdzw = 20.,20., 20., 30.,40.,50.,70.,
         90.,120.,150.,180.,210.,250.,300.,
         400.,500.,600.,700.,900.,1400.,
 /
EOF2
fi

if [ \${ke} -eq 40 ] ; then
cat >> OCECTL  << EOF2
 &ocedzw
 cdzw     = 12.,10.,10.,10.,10.,10.,13.,15.,20.,25.,
            30.,35.,40.,45.,50.,55.,60.,70.,80.,90.,
            100.,110.,120.,130.,140.,150.,170.,180.,190.,200.,
            220.,250.,270.,300.,350.,400.,450.,500.,500.,600.,
 /
EOF2
fi

if [ \${ke} -eq 80 ] ; then
cat >> OCECTL  <<EOF2
 &ocedzw
 cdzw     = 12.,10.,10.,10.,10.,10.,10.,11.,11.,12.,
            13.,13.,14.,14.,15.,16.,16.,17.,18.,19.,
            20.,21.,21.,22.,24.,25.,26.,27.,28.,29.,
            31.,32.,34.,35.,37.,39.,40.,42.,44.,46.,
            48.,50.,53.,55.,58.,60.,63.,66.,69.,72.,
            76.,79.,83.,87.,91.,95.,99.,104.,108.,113.,
            119.,124.,130.,136.,142.,149.,155.,163.,170.,178.,
            186.,195.,204.,213.,223.,233.,244.,255.,267.,279.,
 /
EOF2
fi

# Global integrals/means (code range 513-519)
GLOBAL=\$(seq 513 519)
# Section diagnostics (code range 610-815)
SECTIONS=703,644,645,724,725,684,685,\$(seq 610 765 | grep -v '[0-1,3-9]\$' ;\
         seq 770 815 | grep -v '[3,6-9]\$')
# Region diagnostics (code range 820-963)
REGIONS=\$(seq 820 963)
# MOC diagnostics (code range 100-102)
MOC=\$(seq 93 98),\$(seq 100 102)

# New I/O (ioctl) namelist syntax
#
# iolist(N) = TYPE, FILE, FORMAT, CODE[, CODE...]
# N: unique number from 1 to 100 (not necessarily in sequence)
# TYPE: 99: restart file, 90: snapshot at start, 98: snapshot at end,
#       1: daily mean, 2: monthly mean, 3: annual mean, 4: per timestep,
#       5: 12-hourly mean, 6: 6h mean, 7: 3h mean, 8: 2h mean, 9: 1h mean;
#       adding 100 gives snapshot instead, eg 101: snapshot at end of day
# FILE: file name enclosed by quotes ('...') or double quotes ("...")
# FORMAT: file format: 'nc' NetCDF 1, 'nc2' NetCDF 2/3, 'nc4' NetCDF 4,
#                      'grb' GRIB 1, 'grbsz'/'sz' Compressed GRIB
# CODE...: list of variable code numbers (max. 255 codes)

cat >> OCECTL  <<EOF2
 &ioctl
 iolist(1)=99,'rerun_mpiom','nc4',1,2,3,4,5,7,9,10,13,15,35,36,82,84,99,110,111,141,501,502,503,504
 iolist(11)=2,'\${EXPNO}_mpiom_data_mm.nc','nc2',1,2,3,4,5,6,11,12,13,14,15,16,17,18,21,22,23,24,27,110,111
                                                ,138,139,141,181,182,197
                                                ,219,220,221,222,223,224,226,227,228,229
                                                ,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244
 iolist(12)=1,'\${EXPNO}_mpiom_data_dm.nc','nc2',1,12,13,14,15,17,141,37,38,70,79,52,53
                                                ,113,114,115,116,117,118,245,246,247,248,249,250,251,252
 iolist(21)=2,'\${EXPNO}_mpiom_data_mm.sz','sz',1,2,3,4,5,6,11,12,13,14,15,16,17,18,21,22,23,24,27,110,111
                                                ,138,139,141,181,182,197
                                                ,219,220,221,222,223,224,226,227,228,229
                                                ,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244
 iolist(22)=1,'\${EXPNO}_mpiom_data_dm.sz','sz',1,12,13,14,15,17,141,37,38,70,79,52,53
                                                ,113,114,115,116,117,118,245,246,247,248,249,250,251,252
 iolist(30)=1,'\${EXPNO}_mpiom_timeser_dm.nc','nc2',\$(echo \${MOC} \${GLOBAL} \${SECTIONS} \${REGIONS} | sed 's/ /,/g;s/.{80}/&\n/g')
 iolist(31)=2,'\${EXPNO}_mpiom_monitoring_mm.nc','nc2',652,692,622,672,703,800,804,805,836,837,900,901,950,951,952,953,955,956,958,959,961,962
 iolist(32)=98,'\${EXPNO}_mpiom_map.nc','nc4',511,512
 /
EOF2

cat >> OCECTL <<EOF2
  &forcctl
  cforcdata='${cforcdata}'
  forcing_frequency = \${forcing_frequency}
  forcing_start_time = \${forcing_start}
  lwrite_forcing=.${lwrite_forcing}.
$(if [ -z "$CDIDEF" ]; then
  echo "  lspat_interp_forcing=.${lspat_interp_forcing}."
  echo "  ltime_interp_forcing=.${ltime_interp_forcing}."
fi)
  lperiodic_forcing=.${lperiodic_forcing}.
  ldebug_forcing=.${ldebug_forcing}.
  ldiff_runoff_grid=.${ldiff_runoff_grid}.
  /
EOF2

#
# hamocc
#
if [ \${hamocc} == "true" ] ; then

millennium_ctrl=false
if [ \${millennium_ctrl} = "false" ]; then
 deltacalc=0.0
 deltaorg=0.0
 deltasil=0.0
else
 deltacalc=493.57
 deltaorg=2.457
 deltasil=0.22
fi

#creates daily averages for bgc timeseries
(( TSDT = 86400 / DT ))

# For ioctl namelist syntax, see MPIOM section above

cat > NAMELIST_BGC  << EOF2
 &BGCCTL
 deltacalc   =  \${deltacalc}
 deltaorg    =  \${deltaorg}
 deltasil    =  \${deltasil}
 io_stdo_bgc =  8,
 kchck       =  0,
 isac        =  1,
 mean_2D_freq = 0,
 mean_3D_freq = 0,
 rmasko      = -9e33
 nfreqts1    =  \${TSDT},
 rlonts1     = -20.0,60.0,65.0,-64.0,-175.0,-145.0,-25.0,-140.0
 rlatts1     = 47.0,17.0,10.0,32.0,-53.0,50.0,64.0,5.0
 rdep1ts1    = 80.0, 80.0,80.0,80.0,80.0,80.0,80.0,80.0
 rdep2ts1    = 1000.0,1000.0,1000.0,200.0,1000.0,1000.0,1000.0,1000.0
 rdep3ts1    = 3000.0,3000.0,3000.0,300.0,2000.0,2000.0,2000.0,2000.0
 /
&ioctl
  iolist(1)  = 99, 'rerun_hamocc', 'nc4',
               7,10,11,12,13,14,15,16,17,20,21,22,23,24,27,28,29,30,31,37,38,
               41,44,45,46,47,48,49,50,51,54,55,56,57,58,59,61,62,64,203,204,205
               ! 8,9,18,19,25,26,32,33,35,36,39,40,42,43,52,53
  iolist(11) = 3, '\${EXPNO}_hamocc_data_ym.nc', 'nc2',
               7,10,11,12,14,15,16,17,20,22,23,24,27,29,31,85,86,100,101,158
  iolist(12) = 2, '\${EXPNO}_hamocc_data_mm.nc', 'nc2',
               67,68,72,75,78,81,92,94,95,107,110,111,112,114,115,116,117,
               120,122,123,124,127,129,131,157,159,160,161,162,163,164,165,
               185,186,200,206,207,210,211,214,215,231
  iolist(21) = 3, '\${EXPNO}_hamocc_data_ym.sz', 'sz',
               7,10,11,12,14,15,16,17,20,22,23,24,27,29,31,85,86,100,101,158
  iolist(22) = 2, '\${EXPNO}_hamocc_data_mm.sz', 'sz',
               67,68,72,75,78,81,92,94,95,107,110,111,112,114,115,116,117,
               120,122,123,124,127,129,131,157,159,160,161,162,163,164,165,
               185,186,200,206,207,210,211,214,215,231
  iolist(30) = 2, '\${EXPNO}_hamocc_co2_mm.nc', 'nc2',
               301,303,304,305,306,307,308,309,310,311,312,313,314
  iolist(31) = 2, '\${EXPNO}_hamocc_monitoring_mm.nc', 'nc2',
               500,501,502,503,504,505,506,507
 /
EOF2
#
fi

#=============================================================================
echo "Integration started on \$(date)"
$(case ${queuing} in
  (TWS)
printf '
# with binding
#unset MP_TASK_AFFINITY
#export TARGET_CPU_LIST=-1
#export TARGET_CPU_LIST="0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30"

#export TARGET_CPU_RANGE=-1
#poe ~puetz/bin/launch ${MODDIR}/${MODBIN} -procs $ncpus -hfile /u/m211054/host.list
#poe ~puetz/bin/hybrid_launch ${MODDIR}/${MODBIN} -procs $ncpus -hfile /u/m211054/host.list

# hpmcount
#. /usr/local/ihpct_2.2/env_sh   # RZG
#. /usr/lpp/ppe.hpct/env_sh      # DKRZ
#export LIBPATH=${IHPCT_BASE}/lib64:$LIBPATH
#poe hpmcount ${MODDIR}/${MODBIN} -procs $ncpus -hfile /u/m211054/host.list

if [[ "${profile}" = "yes" ]]; then
  set +e
  trace_running=$(trcstop) || echo trcstop status $?
  set -e
  tprof -usz -p mpiom.x -x poe ${MODDIR}/${MODBIN}
else
  poe ${MODDIR}/${MODBIN}
fi
status=$?
[[ $status -eq 0 ]] || {
   echo " MPIOM did not end correct: exit code is ${status}"
   exit 1
}

if [[ "${profile}" = "yes" ]]; then
  jobid=$(echo $LOADL_STEP_ID | cut -d. -f2)
  profdir=prof_${run_start}.${jobid}
  mkdir ${profdir}
  mv poe.prof mpiom*.hpm mpiom*.viz ${profdir}
  mv mpi_profile* ${profdir} 2>/dev/null || echo "WARNING: Files mpi_profile* not available"
  mv single_trace ${profdir} 2>/dev/null || echo "WARNING: File single_trace not available"
  cp -p OCECTL ${profdir}
  tar cvf ${profdir}/oceout.tar oceout*
fi
'
  ;;

  (none)
printf '
# with mpich2
MPI_BIN=@MPIROOT@/bin
${MPI_BIN}/mpiexec -l -envall -n ${ncpus} ${MODDIR}/${MODBIN}

# with openmpi
#MPI_BIN=@MPIROOT@/bin
##${MPI_BIN}/mpiexec -x MPIOM_THREADS -x FORT_BUFFERED -n ${ncpus} ${MODDIR}/${MODBIN}

# laptop
#MPI_BIN=@MPIROOT@/bin
##${MPI_BIN}/mpiexec -n ${ncpus} ${MODDIR}/${MODBIN}
'
  ;;

  (SGE)
printf '
MPIROOT=@MPIROOT@
MPI_BIN=${MPIROOT}/bin
##${MPI_BIN}/mpiexec --prefix ${MPIROOT} -display-map -x MPIOM_THREADS -x FORT_BUFFERED -np ${ncpus} -bynode ${MODDIR}/${MODBIN}
${MPI_BIN}/mpiexec -np ${ncpus} ${MODDIR}/${MODBIN}

#for interactive use with  NAG compiler and gbd
##${MPI_BIN}/mpiexec -debugger "gdb --args @mpirun@ @mpirun_args@" -debug -host tornado1,tornado1,tornado1,tornado1 -np 4 ${MODDIR}/${MODBIN}
'
  ;;

  (NQS)
printf 'mpirun -np ${ncpus} ${MODDIR}/${MODBIN}'
  ;;

esac)

echo "Integration completed on \$(date)"
#=============================================================================
[[ "\${profile}" = "no" ]] || set +e
[[ -d  \${ARCHIVE}/restart ]] || @MKDIR_P@ \${ARCHIVE}/restart
[[ -d  \${ARCHIVE}/outdata ]] || @MKDIR_P@ \${ARCHIVE}/outdata

model_date=\$(<model_date.asc)
run_start=\$(echo \${model_date} | awk '{print \$1}')
run_end=\$(echo \${model_date} | awk '{print \$2}')
next_run_start=\$(echo \${model_date} | awk '{print \$3}')
timestamp_out=\${run_start}_\${run_end}
timestamp_res=\${run_end}

# restart files
for i in rerun_mpiom rerun_hamocc ; do
[ -r  \$i ] && cp -p \$i \${ARCHIVE}/restart/\${EXPNO}_\${i}_\${timestamp_res}
done

# mpiom files
for i in \$(ls \${EXPNO}_mpiom_* \${EXPNO}_hamocc_*) ; do
[ -r  \$i ] && mv \$i \${ARCHIVE}/outdata/\${i}_\${timestamp_out}
done

if [ \${hamocc} == "true" ] ; then
  tar cvf \${timestamp_out}_hamocc.tar timeser_bgc.nc bgcout oceout
  mv \${timestamp_out}_hamocc.tar \${ARCHIVE}/outdata/\${timestamp_out}_hamocc.tar
  \rm timeser_bgc.nc bgcout oceout
fi
#=============================================================================

#=============================================================================

done

nsdate=\$(echo \${next_run_start} | tr -d '-')
fdate=\$(echo \${model_end}   | tr -d '-')
if [ \${nsdate} -lt \${fdate} ] ; then
  echo "submitting next job"
  ${submit} \${WRKDIR}/${jobname}
fi

exit
EOF1

chmod 755 ${WRKDIR}/${jobname}
printf "\n%s\n" "To start MPIOM experiment ${EXPNO}:"
printf "%s\n" "   cd ${WRKDIR}"
printf "%s\n\n" "   ${submit} ${jobname}"
#-----------------------------------------------------------------------------

exit 0

