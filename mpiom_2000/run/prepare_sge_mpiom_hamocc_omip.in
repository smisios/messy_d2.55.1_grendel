#! /bin/ksh

######################################
#       PREPARE DIRECTORIES
#       AND RUN SCRIPT
######################################

# Supported queuing system
#   one of TWS (Tivoli Workload Scheduler - IBM), SGE (Sun Grid Engine), 
#   NQS (Network Queuing System - NEC SX), or none
# @todo Currently also used for determining use of local files,
# @todo and for choice of parallel environment
queuing=@QUEUE_TYPE@
submit=@QUEUE_SUBMIT@
host=@host@

# computing platform/site settings and compiler:
case $queuing in
  TWS) mode=SMT ; task_geometry=yes ;;
esac
if [ -x `which basename 2>/dev/null | grep -v 'no .* in'` ]; then
  compiler=`basename "@FC@"`
else
  compiler=`echo "@FC@" | sed 's:^.*/::'`
fi

# name of the experiment and grid resolution:
#EXPNO=toyl10
#GRID=TOY
#LEV=L10

#EXPNO=GR30L40
#GRID=GR30
#LEV=L40

#EXPNO=tp10l40
#GRID=TP10
#LEV=L40

EXPNO=tp04l80
GRID=TP04
LEV=L80

# Pick your own working directory, e.g. $WRKSHR or /scratch/local1/$LOGNAME
# Default setting uses a user subdirectory in the default project area.
GROUP=$(id -gn)
WORKDIR=${WORK:-/work/$GROUP}/$LOGNAME

# absolute path to work directory:
EXPDIR=${WORKDIR}/experiments

# absolute path to directory with plenty of space:
ARCDIR=${WORKDIR}/experiments

# absolute path to model binary:
MODDIR=@abs_top_builddir@/bin

# model binary:
MODBIN=mpiom.x

# absolute path to directory with initial data:
INITIAL_DATA=@POOL_ROOT@/MPIOM

nproca=16    # tasks in x direction
nprocb=16    # tasks in y direction
nthread=1    # openmp threads

ncpu=$(( nproca * nprocb * nthread ))

#
# hamocc
#
hamocc=@ENABLE_HAMOCC_RUN@

#
# profiling
#
profile=no

# the directories for the experiment will be created, if not already there
WRKDIR=${EXPDIR}/${EXPNO}
ARCHIVE=${ARCDIR}/${EXPNO}

if [ ! -d $WRKDIR ]; then
    @MKDIR_P@ ${WRKDIR}
    cd ${WRKDIR}
#   cp ${MODDIR}/${MODBIN} ${MODBIN}  &&  MODDIR=${WRKDIR}
fi
if [ ! -d $ARCHIVE ]; then
    @MKDIR_P@ $ARCHIVE/restart
    @MKDIR_P@ $ARCHIVE/outdata
fi

if [ -x '/client/bin/ksh' ]; then
  # where an override for ksh is installed, use that
  shell='/client/bin/ksh'
else
  shell='/bin/ksh'
fi

case ${queuing} in
  (TWS)
  if [[ "${mode}" = "SMT" ]]; then
    tasks_per_node=64
    affinity=cpu
    memory_mb=750
  else
    tasks_per_node=32
    affinity=core
    memory_mb=1500
  fi
  ;;
esac

if [ x$task_geometry = xyes ]; then
  case ${queuing} in

    (TWS)
    # tasks per node in x direction
    if [ $nproca -ge 8 ]; then tilex=8 ;
    else tilex=$nproca ; fi
    (( tiley =  tasks_per_node / tilex ))   # tasks per node in y direction
    if [ $tiley -gt $nprocb ]; then
      if [ $nprocb -ge 8 ]; then tiley=8 ;
      else tiley=$nprocb ; fi
      (( tilex =  tasks_per_node / tiley ))   # tasks per node in y direction
    fi
    export TASK_GEOMETRY=`@abs_top_builddir@/contrib/aix/geometry.pl ${nproca}-${nprocb}-${tasks_per_node}-${tilex}-${tiley}`
    (( ncpu = nproca * nprocb ))
    (( node = ncpu / tasks_per_node )) || { node=1 ; tasks_per_node=${ncpu} ; }

    ;;
  esac
else
  case ${queuing} in
    (TWS)
    ((node = (nproca * nprocb + tasks_per_node - 1) / tasks_per_node ))
    ;;
  esac
fi

jobname=${EXPNO}.job
cat > ${WRKDIR}/${jobname}<<EOF1
#! ${shell}
#-----------------------------------------------------------------------------
$(case ${queuing} in

  (TWS)
printf "
# Version for LoadLeveler
#
# @ shell = ${shell}
# @ class = cluster
# @ job_type = parallel
# @ node_usage= not_shared
# @ rset = rset_mcm_affinity
$(if [ "${task_geometry}" = "yes" ]; then
  echo "# @ mcm_affinity_options = mcm_accumulate"
  echo "# @ task_geometry = $TASK_GEOMETRY"
else
  echo "# @ mcm_affinity_options = mcm_distribute,mcm_mem_req"
  echo "# @ node = ${node}"
  echo "# @ tasks_per_node = ${tasks_per_node}"
fi)
# @ resources = ConsumableMemory(${memory_mb}mb)
# @ task_affinity = ${affinity}(${nthread})
# @ network.MPI = sn_all,not_shared,us
# @ wall_clock_limit = 08:00:00
# @ job_name = ${jobname}
# @ output = \$(job_name).o\$(jobid)
# @ error = \$(job_name).o\$(jobid)
# @ notification = error
# @ queue"
     ;;

  (SGE)
printf "
# Version for Sun GridEngine
#
#$ -S ${shell}
#$ -N ${jobname}
#$ -o \$JOB_NAME.o\$JOB_ID
#$ -j y
#$ -cwd
#$ -pe orte ${ncpu}"
     ;;

  (NQS)
printf  "
# Version for NQSII
#
#PBS -S ${shell}
#PBS -N ${jobname}             # job name
#PBS -l cpunum_prc=${ncpu}     # 8 cpus per node
#PBS -l cputim_job=04:00:00    # 2 h realtime per node
#PBS -l memsz_job=5gb          # 48 GB Memory per node
#PBS -j o                      # join err and out to out"
     ;;

esac)

#-----------------------------------------------------------------------------
#
#                  Job file to run MPIOM with OMIP forcing
#
#-----------------------------------------------------------------------------
#
# If a command has a non-zero exit status, execute ERR trap, if set, and exit
#
set -ex
#
#=============================================================================
$(case ${queuing} in
  (TWS)
printf '
export MEMORY_AFFINITY=MCM
export MP_SINGLE_THREAD=yes

# also see hpct_guide.pdf
export HPM_EVENT_SET=92,127
export HPM_UNIQUE_FILE_NAME=yes
export HPM_AGGREGATE=average.so   # write prof. with per event set averaged values
#export HPM_AGGREGATE=mirror.so   # write unchanged profiles from indiv. tasks
#export HPM_AGGREGATE=single.so   # write profile from one task
                                  # (set via env var HPM_PRINT_TASK)

#export PREATTACH_PROCESSOR_CORE_LIST=AUTO_SELECT
'
  ;;

  (SGE)
printf '
export FORT_BUFFERD="Y"
'
  ;;

  (NQS)
printf '
export MPIPROGINF=ALL_DETAIL
#export F_PROGINF=DETAIL
export F_SETBUF=4096
'
  ;;

esac)

export OMP_NUM_THREADS=1
export MPIOM_THREADS=$nthread
export MPIEXPORT="MPIOM_THREADS"

#
# hamocc
#
hamocc=${hamocc}

#
# profiling
#
profile=${profile}

EXPNO=${EXPNO}
echo "Experiment: \${EXPNO}"

nprocx=$nproca
nprocy=$nprocb
runs=1

(( ncpus = nprocx * nprocy ))
#
echo "   CPUs: \${ncpus} (nprocx: \${nprocx}, nprocy: \${nprocy})"

#-----------------------------------------------------------------------------
#
WRKDIR=${EXPDIR}/\${EXPNO}

# absolute path to model binary and model binary
MODDIR=${MODDIR}
MODBIN=${MODBIN}

# absolute path to directory with plenty of space:
ARCHIVE=${ARCDIR}/\${EXPNO}

# absolute path to directory with initial data:
INITIAL_DATA=${INITIAL_DATA}

# horizontal and vertical resolution
GRID=${GRID}
LEV=${LEV}

#-----------------------------------------------------------------------------
caulapuv=0.005
ibolk=500
ltidal=.false.
iter_sor=300
rtsorpar=-999.
iter_sor_hack=0
rtsorpar_hack=-999.


if [ "\${GRID}" = "GR60" ] ; then
  ie=60
  je=50
  DT=10800
  tp=false

elif [ "\${GRID}" = "TOY" ] ; then
  ie=66
  je=36
  DT=10800
  tp=true

elif [ "\${GRID}" = "GR30" ] ; then
  ie=122
  je=101
  DT=8640
  tp=false

elif [ "\${GRID}" = "GR15" ] ; then
  ie=256
  je=220
  DT=4800
  tp=false

elif [ "\${GRID}" = "TP10" ] ; then
  ie=362
  je=192
  [[ "\${ltidal}" = ".true." ]] && DT=3600 || DT=5400
  tp=true

elif [ "\${GRID}" = "TP04" ] ; then
  ie=802
  je=404
  DT=3600
  tp=true
  caulapuv=0.00375
  iter_sor=300
  rtsorpar=1.916
  iter_sor_hack=10
  rtsorpar_hack=0.7

elif [ "\${GRID}" = "TP6M" ] ; then
  ie=3602
  je=2394
  DT=450
  tp=true
  caulapuv=0.00375
  ibolk=0
fi


if [ "\${LEV}" = "L3" ] ; then
  ke=3

elif [ "\${LEV}" = "L10" ] ; then
  ke=10

elif [ "\${LEV}" = "L20" ] ; then
  ke=20

elif [ "\${LEV}" = "L40" ] ; then
  ke=40

elif [ "\${LEV}" = "L80" ] ; then
  ke=80
fi


#-----------------------------------------------------------------------------
#
cd \${WRKDIR}           #  output and rerun files are written into \$ARCHIVE
#
pwd
#-----------------------------------------------------------------------------
#
# specification of files
#
#-----------------------------------------------------------------------------
#
set +e

#CP='\cp -p'
CP='ln -sf'
$(case "${host}-${compiler}" in
  (x86*-*-linux-*-nagfor|x86*-*-linux-*-nagf95)
    #for NAG compiler we need little endian
    echo "CP='@CDO@ -b 64L copy'"
    ;;
esac)

\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}_arcgri             arcgri
\cp  \${INITIAL_DATA}/\${GRID}/\${GRID}_topo                topo
\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}_anta               anta
\cp \${INITIAL_DATA}/\${GRID}/\${GRID}_BEK                   BEK
chmod u+rw BEK

\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}_GIWIX_OMIP365      GIWIX
\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}_GIWIY_OMIP365      GIWIY
\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}_GITEM_OMIP365      GITEM
\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}_GIPREC_OMIP365     GIPREC
\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}_GISWRAD_OMIP365    GISWRAD
\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}_GITDEW_OMIP365     GITDEW
\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}_GIU10_OMIP365      GIU10
\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}_GICLOUD_OMIP365    GICLOUD
\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}_GIRIV_OMIP365      GIRIV

\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}\${LEV}_INITEM_PHC  INITEM
\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}\${LEV}_INISAL_PHC  INISAL
\$CP  \${INITIAL_DATA}/\${GRID}/\${GRID}\${LEV}_SURSAL_PHC  SURSAL

\$CP  \${INITIAL_DATA}/runoff_obs                         runoff_obs
\$CP  \${INITIAL_DATA}/runoff_pos                         runoff_pos

#-----------------------------------------------------------------------------
if [ \${hamocc} == "true" ] ; then
# monthly mean dust field for hamocc

\cp  \${INITIAL_DATA}/\${GRID}/\${GRID}_MAHOWALDDUST.nc      INPDUST.nc
  chmod u+rw INPDUST.nc
fi

chmod u+rw arcgri topo anta BEK GI* INITEM INISAL SURSAL

#-----------------------------------------------------------------------------
set -e

for lll in 1 ; do

typeset -Z4 YEAR=\$(cat year.asc 2> /dev/null || echo 0) ; export YEAR
echo \$YEAR
STA=3 ; export STA
RES=0 ; export RES

if [ \${YEAR} -eq 0 ] ; then
STA=2 ; export STA
RES=1 ; export RES
fi

#
# istart=0 >NEWSTART USE ONLY FOR COMPLETELY NEW SETUP (new topography etc!!!)
# istart=1 start from horizonally uniform t,s profiles
# istart=2 start from levitus
# istart=3 start from existing restart files Z37000, Z38000 (default)
#
# i3drest=0 NO 3d-restoring (default)
# i3drest=1 restoring to annual mean t,s
# i3drest=2 restoring to monthluy mean t,s
#
# Advection schemes
# iocad=3 ADPO (default)
# iocad=4 obsolete, use iocad=3 and ibbl_transport=1
# iocad=5 ADFS
#
# BBL transport (slope convection)
# ibbl_transport=1  (default)
# nfixYearLen = -1, 365 or 360 ; default -1 for 365/366 (leap years) 
#
# ltidal=.true.  enables eph.tidal sub model (default=false)
#
# imean is usually superceded by ioctl namelist below
# (imean=0 no output
#  imean=1 daily average of output fields
#  imean=2 monthly average of output fields (default)
#  imean=3 yearly average of output fields
#  imean=4 every time step)
#
# nyears: number of years to be simulated (default=0)
# nmonths: number of months to be simulated (default=1)
#
cat > OCECTL  << EOF
&ocedim
 ie_g = \${ie}
 je_g = \${je}
 ke = \${ke}
 lbounds_exch_tp =  .\${tp}.
/
&nprocs
 nprocx=\${nprocx}
 nprocy=\${nprocy}
/
 &ocectl
 dt      = \${DT}
 caulapts= 0.
 caulapuv= \${caulapuv}
 aus     = 0.
 cah00   = 1000.
 ibolk   = \${ibolk}
 dv0     = 0.2e-2
 av0     = 0.2e-2
 cwt     = 0.5e-3
 cwa     = 0.75e-3
 cstabeps= 0.03
 dback   = 1.05e-5
 aback   = 5.e-5
 crelsal = 3.3e-7
 creltem = 0.
 cdvocon = 0.1
 cavocon = 0.0
 nfixYearLen = 365
 ltidal  = \${ltidal}
 nyears  = 0
 nmonts  = 12
 ndays   = 0
 imean   = 0       ! 2 for monthly mean output
 istart  = \${STA}
 i3drest = \${RES}
 ihalo_sor   = 2
 imod    = 1
 lmpitype = .false.
 lnonblock = .true.
 icontro = 0
 lundelayed_momentum_advection = .false.
 rleadclose=0.25,1,0
 h0=0.2
 iter_sor = \${iter_sor}
 rtsorpar = \${rtsorpar}
 iter_sor_hack = \${iter_sor_hack}
 rtsorpar_hack = \${rtsorpar_hack}
 /
EOF

if [ \${ke} -eq 3 ] ; then
cat >> OCECTL  << EOF2
 &ocedzw
 cdzw     = 12.,10.,5000.,
 /
EOF2
fi


if [ \${ke} -eq 10 ] ; then
cat >> OCECTL  << EOF2
&ocedzw
  cdzw = 40., 50., 90., 160., 270., 390., 550., 900., 1300., 2300.
 /
EOF2
fi


if [ \${ke} -eq 20 ] ; then
cat >> OCECTL  << EOF2
 &ocedzw
  cdzw = 20.,20., 20., 30.,40.,50.,70.,
         90.,120.,150.,180.,210.,250.,300.,
         400.,500.,600.,700.,900.,1400.,
 /
EOF2
fi

if [ \${ke} -eq 40 ] ; then
cat >> OCECTL  << EOF2
 &ocedzw
 cdzw     = 12.,10.,10.,10.,10.,10.,13.,15.,20.,25.,
            30.,35.,40.,45.,50.,55.,60.,70.,80.,90.,
            100.,110.,120.,130.,140.,150.,170.,180.,190.,200.,
            220.,250.,270.,300.,350.,400.,450.,500.,500.,600.,
 /
EOF2
fi

if [ \${ke} -eq 80 ] ; then
cat >> OCECTL  <<EOF2
 &ocedzw
 cdzw     = 12.,10.,10.,10.,10.,10.,10.,11.,11.,12.,
            13.,13.,14.,14.,15.,16.,16.,17.,18.,19.,
            20.,21.,21.,22.,24.,25.,26.,27.,28.,29.,
            31.,32.,34.,35.,37.,39.,40.,42.,44.,46.,
            48.,50.,53.,55.,58.,60.,63.,66.,69.,72.,
            76.,79.,83.,87.,91.,95.,99.,104.,108.,113.,
            119.,124.,130.,136.,142.,149.,155.,163.,170.,178.,
            186.,195.,204.,213.,223.,233.,244.,255.,267.,279.,
 /
EOF2
fi


# New I/O (ioctl) namelist syntax
#
# iolist(N) = TYPE, FILE, FORMAT, CODE[, CODE...]
# N: unique number from 1 to 100 (not necessarily in sequence)
# TYPE: 99: restart file, 90: snapshot at start, 98: snapshot at end,
#       1: daily mean, 2: monthly mean, 3: annual mean, 4: per timestep,
#       5: 12-hourly mean, 6: 6h mean, 7: 3h mean, 8: 2h mean, 9: 1h mean;
#       adding 100 gives snapshot instead, eg 101: snapshot at end of day
# FILE: file name enclosed by quotes ('...') or double quotes ("...")
# FORMAT: file format: 'nc' NetCDF 1, 'nc2' NetCDF 2/3, 'nc4' NetCDF 4,
#                      'grb' GRIB 1, 'grbsz'/'sz' Compressed GRIB
# CODE...: list of variable code numbers (max. 255 codes)

cat >> OCECTL  <<EOF2
 &ioctl
 iolist(1)=99,'rerun_mpiom','nc4',3,4,2,5,1,82,13,15,35,36,141,501,502,503,504,111,110,7,99,9,10,84
 iolist(2)=2,'monthly_mpiom','sz',3,4,2,5,7,67,1,70,79,13,15,35,36,141,142,143,176,177,147,146,65,27,183
! iolist(3)=3,'annual_mpiom','sz',3,4,2,5,7,67,1,70,79,13,15,35,36,141,142,143,176,177,147,146,65,27,183
! iolist(4)=1,'daily_mpiom','sz',1,2
! iolist(5)=4,'timestep_mpiom','sz',1
 iolist(10)=1,'moc_mpiom','nc2',100,101,102
 iolist(99)=1,'timeser_mpiom','nc2',\$(z=\`seq 610 765 | grep -v '[0-1,3-9]\$' ; seq 770 815 | grep -v '[3,6-9]\$' ; seq 820 963\` ; echo \$z | sed 's/ /,/g;s/.{80}/&\n/g')
! iolist(100)=101,'secmap_mpiom','nc2',512
 /
EOF2

cat >> OCECTL  <<EOF2
 &forcctl
 /
EOF2

#
# hamocc
#
if [ \${hamocc} == "true" ] ; then

# mean_2D/3D_freq is usually superceded by ioctl namelist below
# (mean_2D/3D_freq=0 no output
#  mean_2D/3D_freq=1 daily average of output fields
#  mean_2D/3D_freq=2 monthly average of output fields (default for mean_2D_freq)
#  mean_2D/3D_freq=3 yearly average of output fields (default for mean_3D_freq)
#  mean_2D/3D_freq=4 every time step)

#creates daily averages for bgc timeseries
(( TSDT = 86400 / DT ))

# For ioctl namelist syntax, see MPIOM section above

cat > NAMELIST_BGC  << EOF2
 &BGCCTL
 io_stdo_bgc =  8,
 kchck       =  0,
 isac        =  1,
 mean_2D_freq = 0,
 mean_3D_freq = 0,
 rmasko      = -9e33
 nfreqts1    =  \${TSDT},
 rlonts1     = -20.0,60.0,65.0,-64.0,-175.0,-145.0,-25.0,-140.0
 rlatts1     = 47.0,17.0,10.0,32.0,-53.0,50.0,64.0,5.0
 rdep1ts1    = 80.0, 80.0,80.0,80.0,80.0,80.0,80.0,80.0
 rdep2ts1    = 1000.0,1000.0,1000.0,200.0,1000.0,1000.0,1000.0,1000.0
 rdep3ts1    = 3000.0,3000.0,3000.0,300.0,2000.0,2000.0,2000.0,2000.0
 /
&ioctl
  iolist(1)  = 99, 'rerun_hamocc', 'nc2',
               7,10,11,12,13,14,15,16,17,20,21,22,23,24,27,28,29,30,31,37,38,
               41,44,45,46,47,48,49,50,51,54,55,56,57,58,59,61,62,64
               ! 8,9,18,19,25,26,32,33,35,36,39,40,42,43,52,53
  iolist(11) = 3, '\${EXPNO}_hamocc_data_ym.nc', 'nc2',
               7,10,11,12,14,15,16,17,20,22,23,24,27,29,31,85,86,100,101,158
  iolist(12) = 2, '\${EXPNO}_hamocc_data_mm.nc', 'nc2',
               67,68,72,75,78,81,92,94,95,107,110,111,112,114,115,116,117,
               120,122,123,124,127,129,131,157,159,160,161,162,163,164,165,
               185,186,200,206,207,210,211,214,215,231
  iolist(21) = 3, '\${EXPNO}_hamocc_data_ym.sz', 'sz',
               7,10,11,12,14,15,16,17,20,22,23,24,27,29,31,85,86,100,101,158
  iolist(22) = 2, '\${EXPNO}_hamocc_data_mm.sz', 'sz',
               67,68,72,75,78,81,92,94,95,107,110,111,112,114,115,116,117,
               120,122,123,124,127,129,131,157,159,160,161,162,163,164,165,
               185,186,200,206,207,210,211,214,215,231
  iolist(31) = 1, '\${EXPNO}_hamocc_monitoring_dm.nc', 'nc2',
               500,501,502,503,504,505,506,507
! iolist(2)=2,'bgcmean_2d','sz',65,66,67,29,68,69,70,71,72,73,74,61,62,64,75,76,77,78,79,80,81,82,83,84,85,86,91,92,93,94,95,96,97
! iolist(3)=3,'bgcmean_3d','sz',11,14,15,31,12,10,7,16,17,24,27
! iolist(4)=2,'bgcmean_bioz','sz',22,23,100,11,12,31,14,10,15,7,16
! iolist(5)=3,'bgcmean_sed','sz',51,54,55,56,57,58,59,38,44,41,45
! iolist(51)=3,'hamocc_cmip5_3d_yr.grbsz','sz',7,10,11,12,14,15,16,17,20,22,23,24,25,29,31,108
! iolist(52)=3,'hamocc_cmip5_3dt_yr.grbsz','sz',85,86,100,101
! iolist(53)=2,'hamocc_cmip5_3d_mon.grbsz','sz',7,10,11,12,14,15,31,109
! iolist(54)=2,'hamocc_cmip5_3dt_mon.grbsz','sz',85,86,100
! iolist(55)=2,'hamocc_cmip5_2d_mon.grbsz','sz',61,66,67,68,72,75,78,91,94,95,106,110
! iolist(56)=1,'hamocc_timeser_day.nc','nc2',500,501,502,503,504,505,506,507
 /
EOF2
#
fi

#=============================================================================
echo "Integration started on \$(date)"
$(case ${queuing} in
  (TWS)
printf '
# with binding
#unset MP_TASK_AFFINITY
#export TARGET_CPU_LIST=-1
#export TARGET_CPU_LIST="0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30"

#export TARGET_CPU_RANGE=-1
#poe ~puetz/bin/launch ${MODDIR}/${MODBIN} -procs $ncpus -hfile /u/m211054/host.list
#poe ~puetz/bin/hybrid_launch ${MODDIR}/${MODBIN} -procs $ncpus -hfile /u/m211054/host.list

# hpmcount
#. /usr/local/ihpct_2.2/env_sh   # RZG
#. /usr/lpp/ppe.hpct/env_sh      # DKRZ
#export LIBPATH=${IHPCT_BASE}/lib64:$LIBPATH
#poe hpmcount ${MODDIR}/${MODBIN} -procs $ncpus -hfile /u/m211054/host.list

if [[ "${profile}" = "yes" ]]; then
  set +e
  trace_running=$(trcstop) || echo trcstop status $?
  tprof -usz -p mpiom.x -x poe ${MODDIR}/${MODBIN}
  set -e
else
  poe ${MODDIR}/${MODBIN}
fi
echo status $?

if [[ "${profile}" = "yes" ]]; then
  jobid=$(echo $LOADL_STEP_ID | cut -d. -f2)
  profdir=prof_${YEAR}.${jobid}
  mkdir ${profdir}
  mv poe.prof mpi_profile* single_trace mpiom.hpm mpiom.viz ${profdir}
  cp -p OCECTL ${profdir}
  tar cvf ${profdir}/oceout.tar oceout*
  rm oceout*
fi
'
  ;;

  (none)
printf '
# with mpich2
MPI_BIN=@MPIROOT@/bin
${MPI_BIN}/mpiexec -l -envall -n ${ncpus} ${MODDIR}/${MODBIN}

# with openmpi
#MPI_BIN=@MPIROOT@/bin
##${MPI_BIN}/mpiexec -x MPIOM_THREADS -x FORT_BUFFERED -n ${ncpus} ${MODDIR}/${MODBIN}

# laptop
#MPI_BIN=@MPIROOT@/bin
##${MPI_BIN}/mpiexec -n ${ncpus} ${MODDIR}/${MODBIN}
'
  ;;

  (SGE)
printf '
MPIROOT=@MPIROOT@
MPI_BIN=${MPIROOT}/bin
##${MPI_BIN}/mpiexec --prefix ${MPIROOT} -display-map -x MPIOM_THREADS -x FORT_BUFFERED -np ${ncpus} -bynode ${MODDIR}/${MODBIN}
${MPI_BIN}/mpiexec -np ${ncpus} ${MODDIR}/${MODBIN}

#for interactive use with  NAG compiler and gbd
##${MPI_BIN}/mpiexec -debugger "gdb --args @mpirun@ @mpirun_args@" -debug -host tornado1,tornado1,tornado1,tornado1 -np 4 ${MODDIR}/${MODBIN}
'
  ;;

  (NQS)
printf 'mpirun -np ${ncpus} ${MODDIR}/${MODBIN}'
  ;;

esac)

echo "Integration completed on \$(date)"
#=============================================================================
[[ "\${profile}" = "no" ]] || set +e
[[ -d  \${ARCHIVE}/restart ]] || @MKDIR_P@ \${ARCHIVE}/restart
[[ -d  \${ARCHIVE}/outdata ]] || @MKDIR_P@ \${ARCHIVE}/outdata

# restart files
[ -r  rerun_mpiom ] && cp -p rerun_mpiom \${ARCHIVE}/restart/rerun_mpiom_\${YEAR}
[ -r  rerun_hamocc ] && cp -p rerun_hamocc \${ARCHIVE}/restart/rerun_hamocc_\${YEAR}

# output files
[ -r  timestep_mpiom ] && mv timestep_mpiom \${ARCHIVE}/outdata/timestep_mpiom_\${YEAR}
[ -r  daily_mpiom ] && mv daily_mpiom \${ARCHIVE}/outdata/daily_mpiom_\${YEAR}
[ -r  monthly_mpiom ] && mv monthly_mpiom \${ARCHIVE}/outdata/monthly_mpiom_\${YEAR}
[ -r  annual_mpiom ] && mv annual_mpiom \${ARCHIVE}/outdata/annual_mpiom_\${YEAR}
[ -r  timeser_mpiom ] && mv timeser_mpiom \${ARCHIVE}/outdata/timeser_mpiom_\${YEAR}
[ -r  moc_mpiom ] && mv moc_mpiom \${ARCHIVE}/outdata/moc_mpiom_\${YEAR}

[ -r  bgcmean_2d ] && mv bgcmean_2d \${ARCHIVE}/outdata/bgcmean_2d_\${YEAR}
[ -r  bgcmean_3d ] && mv bgcmean_3d \${ARCHIVE}/outdata/bgcmean_3d_\${YEAR}
[ -r  bgcmean_bioz ] && mv bgcmean_bioz \${ARCHIVE}/outdata/bgcmean_bioz_\${YEAR}
[ -r  bgcmean_sed ] && mv bgcmean_sed \${ARCHIVE}/outdata/bgcmean_sed_\${YEAR}

[ -r  fort.71 ] && mv fort.71 \${ARCHIVE}/outdata/\${EXPNO}_\${YEAR}.ext4

if [ \${hamocc} == "true" ] ; then
tar cvf \${YEAR}_hamocc.tar timeser_bgc.nc bgcout oceout
mv \${YEAR}_hamocc.tar \${ARCHIVE}/outdata/\${YEAR}_hamocc.tar
\rm timeser_bgc.nc
fi
#=============================================================================

(( YEAR = YEAR + 1 ))
\rm -f year.asc
echo \$YEAR > year.asc

done

if [ \${YEAR} -lt \$runs ] ; then
  echo "submitting next job"
  ${submit} \${WRKDIR}/${jobname}
fi

exit
EOF1

chmod 755 ${WRKDIR}/${jobname}
printf "\n%s\n" "To start MPIOM experiment ${EXPNO}:"
printf "%s\n" "   cd ${WRKDIR}"
printf "%s\n\n" "   ${submit} ${jobname}"
#-----------------------------------------------------------------------------

exit 0

